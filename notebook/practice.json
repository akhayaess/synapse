{
	"name": "practice",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "65360813-11d4-4dd9-8300-a89bcd0aab31"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/0328350c-f7d1-4ca4-8da5-485b8f684719/resourceGroups/RG_DA_1/providers/Microsoft.Synapse/workspaces/asa-rg-da-1/bigDataPools/sparkpool1",
				"name": "sparkpool1",
				"type": "Spark",
				"endpoint": "https://asa-rg-da-1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 60
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#importing libraries\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from azure.storage.blob import BlobServiceClient\r\n",
					"from notebookutils import mssparkutils\r\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#creating session\r\n",
					"spark=SparkSession.builder.getOrCreate()"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#creating manual dataframe\r\n",
					"# data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)\r\n",
					"# df=spark.createDataFrame(data,[\"Name\",\"Age\"])"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# #filtering , and iterating dataframes\r\n",
					"# df=df.filter(df[\"Age\"]>26)\r\n",
					"# df_selected=df.select(\"Name\")\r\n",
					"# df_list=df_selected.collect()\r\n",
					"# for i in df_list:\r\n",
					"#     print(i.Name)\r\n",
					"# # display(df.collect())"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# #copying files in same RG but different containers- we can do it directly\r\n",
					"# StorageAccountname=\"hotandcoldstorage\"\r\n",
					"# StoragelinkedService=\"AzureDataLakeStorage2\"\r\n",
					"# FromContainer=\"powerbi\"\r\n",
					"# ToContainer=\"powerbiorc\""
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#To access azure blob storage from synapse spark for same RG\r\n",
					"#we don't require to create mount point within same RG, we can fetch source and target by using abfss\r\n",
					"#source\r\n",
					"# x=\"abfss://powerbi@hotandcoldstorage.dfs.core.windows.net/*2023*\"\r\n",
					"# # print(x)\r\n",
					"# df=spark.read.parquet(x)\r\n",
					"# # display(df)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#target\r\n",
					"# y=\"abfss://powerbiorc@hotandcoldstorage.dfs.core.windows.net/demo\"\r\n",
					"# df.write.format(\"parquet\").save(y)"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#To list the contents of path\r\n",
					"# mssparkutils.fs.ls(\"abfss://powerbi@hotandcoldstorage.dfs.core.windows.net/\")"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Returns file properties including file name, file path, file size, and whether it is a directory and a file.\r\n",
					"\r\n",
					"# files = mssparkutils.fs.ls(\"abfss://powerbi@hotandcoldstorage.dfs.core.windows.net/\")\r\n",
					"# yt=[]\r\n",
					"# for file in files:\r\n",
					"#     yt.append(file.name)\r\n",
					"#     # print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
					"# print(yt)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Copies a file or directory. Supports copy across file systems.\r\n",
					"\r\n",
					"# mssparkutils.fs.cp('source file or directory', 'destination file or directory', True)\r\n",
					"# Set the third parameter as True to copy all files and directories recursively"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#connect to database\r\n",
					"# server = \"wdp-mit-cdt-weu-sql.database.windows.net\"\r\n",
					"# database = \"wdp-mit-metadata\"\r\n",
					"# username = \"mit_admin\"\r\n",
					"# from notebookutils import mssparkutils\r\n",
					"# # password = get_secret_value('wdp-mit-cdt-weu-sql-metadata-password')\r\n",
					"# password = mssparkutils.credentials.getSecret('wdp-mit-cdt-weu-kv-01','wdp-mit-cdt-weu-sql-metadata-password')\r\n",
					"# print(password)\r\n",
					"# jdbc_url = f\"jdbc:sqlserver://{server};database={database};user={username};password={password}\"\r\n",
					"# audit_data = spark.read \\\r\n",
					"#     .format(\"jdbc\") \\\r\n",
					"#     .option(\"url\", jdbc_url) \\\r\n",
					"#     .option(\"dbtable\", \"dbo.audit_log\") \\\r\n",
					"#     .load()\r\n",
					"# display(audit_data)\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#To access azure blob storage from synapse spark for different RG\r\n",
					"\r\n",
					"# Azure storage access info\r\n",
					"# blob_account_name = 'Your account name' # replace with your blob name\r\n",
					"# blob_container_name = 'Your container name' # replace with your container name\r\n",
					"# blob_relative_path = 'Your path' # replace with your relative folder path\r\n",
					"# linked_service_name = 'Your linked service name' # replace with your linked service name\r\n",
					"\r\n",
					"# blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
					"\r\n",
					"# # Allow SPARK to access from Blob remotely\r\n",
					"\r\n",
					"# wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
					"\r\n",
					"# spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"# print('Remote blob path: ' + wasb_path)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# x=\"abfss://root@hotandcoldstorage.dfs.core.windows.net/demo\"\r\n",
					"# df=spark.read.csv(x,header=True,inferSchema=True)\r\n",
					"# display(df)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# connect to database\r\n",
					"server = \"server-rgda1.database.windows.net\"\r\n",
					"database = \"azuresqldb_rgda1\"\r\n",
					"username = \"rgda1\"\r\n",
					"from notebookutils import mssparkutils\r\n",
					"# password = get_secret_value('wdp-mit-cdt-weu-sql-metadata-password')\r\n",
					"password = \"esspl@123\"\r\n",
					"jdbc_url = f\"jdbc:sqlserver://{server};database={database};user={username};password={password}\"\r\n",
					"datetime_data = spark.read \\\r\n",
					"    .format(\"jdbc\") \\\r\n",
					"    .option(\"url\", jdbc_url) \\\r\n",
					"    .option(\"dbtable\", \"dbo.datetime_data\") \\\r\n",
					"    .load()\r\n",
					""
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(datetime_data)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"startDate=\"2023-07-10 14:20:13\"\r\n",
					"endDate=\"2023-07-10 14:20:17\""
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"datetime_data_df=datetime_data.filter(\r\n",
					"    (datetime_data.DateTime > startDate) & (datetime_data.DateTime<= endDate)\r\n",
					"    )"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"datetime_data_df.show()"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"current_date=\"2023-07-10 14:20:17\""
				]
			}
		]
	}
}